{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # type: ignore\n",
    "import torch.nn as nn # type: ignore\n",
    "from torch.utils.data import DataLoader # type: ignore\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts # type: ignore\n",
    "from torch.utils.tensorboard import SummaryWriter # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "from model import PitcherTransformerModel, BatterTransformerModel, PretrainingTransformer\n",
    "from dataset import MLB_Batter_Dataset, MLB_Pitcher_Dataset, PretrainingDataset\n",
    "from config import pitcher_features, batter_features\n",
    "from utils import compute_metrics\n",
    "from datetime import datetime\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Pretraining Training Loop\n",
    "##############################################\n",
    "def train_pretraining_model(model, dataloader, num_epochs=10, lr=1e-3, device='cpu'):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            # batch is a sequence: [batch, seq_len, input_dim]\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            reconstructions = model(batch)\n",
    "            loss = criterion(reconstructions, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * batch.size(0)\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        print(f\"Pretraining Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}\")\n",
    "    return model\n",
    "\n",
    "##############################################\n",
    "# Training Loop for Masked Pretraining\n",
    "##############################################\n",
    "def train_masked_pretraining_model(model, dataloader, num_epochs=10, lr=1e-3, device='cpu'):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # Use MSELoss with no reduction so we can average only over masked tokens\n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            # Assume each batch is a tensor of shape [batch, seq_len, input_dim]\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed, mask = model(batch)\n",
    "            # Compute reconstruction loss only over masked positions\n",
    "            loss = criterion(reconstructed, batch)  # [batch, seq_len, input_dim]\n",
    "            # Create a mask for the loss: expand the mask to cover the feature dimension\n",
    "            loss_mask = mask.unsqueeze(-1).expand_as(loss)\n",
    "            if loss_mask.sum() > 0:\n",
    "                loss = loss[loss_mask].mean()\n",
    "            else:\n",
    "                loss = loss.mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * batch.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader, num_epochs=10, lr=1e-3, device='cpu'):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n",
    "    model_dir = os.path.join(\"models\", f\"experiment1_{datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n",
    "\n",
    "    # create model_dir if it does not exist\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "        \n",
    "    log_dir = os.path.join(\"runs\", f\"experiment1_{datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_path = os.path.join(model_dir, \"best_model.pth\")\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in train_dataloader:\n",
    "            primary_seq, opponent_seq, targets = batch\n",
    "            primary_seq = primary_seq.to(device)\n",
    "            opponent_seq = opponent_seq.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(primary_seq, opponent_seq)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step(epoch + global_step / len(train_dataloader))\n",
    "            \n",
    "            running_loss += loss.item() * primary_seq.size(0)\n",
    "            global_step += 1\n",
    "        train_loss = running_loss / len(train_dataloader.dataset)\n",
    "        \n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        all_preds, all_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                primary_seq, opponent_seq, targets = batch\n",
    "                primary_seq = primary_seq.to(device)\n",
    "                opponent_seq = opponent_seq.to(device)\n",
    "                targets = targets.to(device)\n",
    "                outputs = model(primary_seq, opponent_seq)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_running_loss += loss.item() * primary_seq.size(0)\n",
    "                all_preds.append(outputs)\n",
    "                all_targets.append(targets)\n",
    "        val_loss = val_running_loss / len(val_dataloader.dataset)\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_targets = torch.cat(all_targets)\n",
    "        mae, rmse, r2 = compute_metrics(all_preds, all_targets)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "              f\"MAE: {mae:.4f}, RMSE: {rmse:.4f}, R2: {r2:.4f}\")\n",
    "        \n",
    "        # Log metrics to TensorBoard\n",
    "        writer.add_scalars(\"Loss\", {\"train\": train_loss, \"val\": val_loss}, epoch)\n",
    "        writer.add_scalar(\"MAE/val\", mae, epoch)\n",
    "        writer.add_scalar(\"RMSE/val\", rmse, epoch)\n",
    "        writer.add_scalar(\"R2/val\", r2, epoch)\n",
    "        writer.add_scalar(\"LearningRate\", optimizer.param_groups[0]['lr'], epoch)\n",
    "        writer.flush()\n",
    "        \n",
    "        # Checkpointing: Save model if validation loss improved\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "            }, best_model_path)\n",
    "            print(f\"--> Saved best model with val_loss: {val_loss:.4f} at epoch {epoch+1}\")\n",
    "    \n",
    "    writer.close()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batter data: (122263, 15)\n",
      "Validation batter data: (13585, 15)\n",
      "Train pitcher data: (8459, 15)\n",
      "Validation pitcher data: (940, 15)\n"
     ]
    }
   ],
   "source": [
    "batter_df_original = pd.read_csv('data/train_batters.csv')[batter_features + ['opp_starting_pitcher', 'PLAYER-ID', 'DATE', 'TEAM', 'OPPONENT', 'next_target_H']]\n",
    "pitcher_df_original = pd.read_csv('data/train_pitchers.csv')[pitcher_features + ['PLAYER-ID', 'DATE', 'TEAM', 'OPPONENT', 'next_target_SO']]\n",
    "\n",
    "# Assuming batter_df_original and pitcher_df_original are your full datasets.\n",
    "n_batter = batter_df_original.shape[0]\n",
    "n_pitcher = pitcher_df_original.shape[0]\n",
    "\n",
    "train_batter_df = batter_df_original.head(int(n_batter * 0.9))\n",
    "val_batter_df = batter_df_original.tail(n_batter - train_batter_df.shape[0])\n",
    "\n",
    "train_pitcher_df = pitcher_df_original.head(int(n_pitcher * 0.9))\n",
    "val_pitcher_df = pitcher_df_original.tail(n_pitcher - train_pitcher_df.shape[0])\n",
    "\n",
    "print(\"Train batter data:\", train_batter_df.shape)\n",
    "print(\"Validation batter data:\", val_batter_df.shape)\n",
    "print(\"Train pitcher data:\", train_pitcher_df.shape)\n",
    "print(\"Validation pitcher data:\", val_pitcher_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Pretraining Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pretraining dataset and dataloader\n",
    "batter_pretrain_dataset = PretrainingDataset(\n",
    "    train_batter_df, \n",
    "    player_id_col='PLAYER-ID', \n",
    "    date_col='DATE',\n",
    "    useful_stats_cols=batter_features, \n",
    "    sequence_length=3\n",
    "    )\n",
    "\n",
    "pitcher_pretrain_dataset = PretrainingDataset(\n",
    "    df=train_pitcher_df,\n",
    "    player_id_col='PLAYER-ID',\n",
    "    date_col='DATE',\n",
    "    useful_stats_cols=pitcher_features,\n",
    "    sequence_length=3\n",
    ")\n",
    "\n",
    "batter_pretrain_loader = DataLoader(batter_pretrain_dataset, batch_size=32, shuffle=True)\n",
    "pitcher_pretrain_loader = DataLoader(pitcher_pretrain_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pretraining Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and pretrain the autoencoder model\n",
    "batter_pretrain_model = PretrainingTransformer(\n",
    "    input_dim=len(batter_features), \n",
    "    model_dim=64, \n",
    "    n_heads=4, \n",
    "    num_layers=2, \n",
    "    sequence_length=3\n",
    "    )\n",
    "\n",
    "# Instantiate the PretrainingTransformer for pitchers.\n",
    "pitcher_pretrain_model = PretrainingTransformer(\n",
    "    input_dim=len(pitcher_features),\n",
    "    model_dim=64,\n",
    "    n_heads=4,\n",
    "    num_layers=2,\n",
    "    sequence_length=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Pitcher Pretraining....\n",
      "Pretraining Epoch 1/100 - Loss: 642.0559\n",
      "Pretraining Epoch 2/100 - Loss: 479.2214\n",
      "Pretraining Epoch 3/100 - Loss: 314.1846\n",
      "Pretraining Epoch 4/100 - Loss: 185.2379\n",
      "Pretraining Epoch 5/100 - Loss: 101.8665\n",
      "Pretraining Epoch 6/100 - Loss: 59.9848\n",
      "Pretraining Epoch 7/100 - Loss: 29.7881\n",
      "Pretraining Epoch 8/100 - Loss: 15.6668\n",
      "Pretraining Epoch 9/100 - Loss: 12.0270\n",
      "Pretraining Epoch 10/100 - Loss: 5.3355\n",
      "Pretraining Epoch 11/100 - Loss: 3.4583\n",
      "Pretraining Epoch 12/100 - Loss: 2.7421\n",
      "Pretraining Epoch 13/100 - Loss: 1.8877\n",
      "Pretraining Epoch 14/100 - Loss: 2.9260\n",
      "Pretraining Epoch 15/100 - Loss: 0.8205\n",
      "Pretraining Epoch 16/100 - Loss: 1.8411\n",
      "Pretraining Epoch 17/100 - Loss: 0.5377\n",
      "Pretraining Epoch 18/100 - Loss: 0.5478\n",
      "Pretraining Epoch 19/100 - Loss: 1.3938\n",
      "Pretraining Epoch 20/100 - Loss: 0.3483\n",
      "Pretraining Epoch 21/100 - Loss: 0.5432\n",
      "Pretraining Epoch 22/100 - Loss: 2.0234\n",
      "Pretraining Epoch 23/100 - Loss: 0.5058\n",
      "Pretraining Epoch 24/100 - Loss: 2.4208\n",
      "Pretraining Epoch 25/100 - Loss: 0.2315\n",
      "Pretraining Epoch 26/100 - Loss: 0.2729\n",
      "Pretraining Epoch 27/100 - Loss: 0.5791\n",
      "Pretraining Epoch 28/100 - Loss: 0.2001\n",
      "Pretraining Epoch 29/100 - Loss: 0.5911\n",
      "Pretraining Epoch 30/100 - Loss: 0.1576\n",
      "Pretraining Epoch 31/100 - Loss: 0.1914\n",
      "Pretraining Epoch 32/100 - Loss: 0.2830\n",
      "Pretraining Epoch 33/100 - Loss: 1.2962\n",
      "Pretraining Epoch 34/100 - Loss: 0.2837\n",
      "Pretraining Epoch 35/100 - Loss: 0.2124\n",
      "Pretraining Epoch 36/100 - Loss: 0.3881\n",
      "Pretraining Epoch 37/100 - Loss: 0.6444\n",
      "Pretraining Epoch 38/100 - Loss: 0.3195\n",
      "Pretraining Epoch 39/100 - Loss: 0.1857\n",
      "Pretraining Epoch 40/100 - Loss: 0.3891\n",
      "Pretraining Epoch 41/100 - Loss: 0.2765\n",
      "Pretraining Epoch 42/100 - Loss: 0.6772\n",
      "Pretraining Epoch 43/100 - Loss: 0.0866\n",
      "Pretraining Epoch 44/100 - Loss: 0.2355\n",
      "Pretraining Epoch 45/100 - Loss: 0.9436\n",
      "Pretraining Epoch 46/100 - Loss: 0.1239\n",
      "Pretraining Epoch 47/100 - Loss: 0.0737\n",
      "Pretraining Epoch 48/100 - Loss: 0.3959\n",
      "Pretraining Epoch 49/100 - Loss: 0.4592\n",
      "Pretraining Epoch 50/100 - Loss: 0.4259\n",
      "Pretraining Epoch 51/100 - Loss: 0.0993\n",
      "Pretraining Epoch 52/100 - Loss: 0.0326\n",
      "Pretraining Epoch 53/100 - Loss: 0.4025\n",
      "Pretraining Epoch 54/100 - Loss: 0.2181\n",
      "Pretraining Epoch 55/100 - Loss: 0.0359\n",
      "Pretraining Epoch 56/100 - Loss: 0.1457\n",
      "Pretraining Epoch 57/100 - Loss: 0.9840\n",
      "Pretraining Epoch 58/100 - Loss: 0.3271\n",
      "Pretraining Epoch 59/100 - Loss: 0.0813\n",
      "Pretraining Epoch 60/100 - Loss: 0.1111\n",
      "Pretraining Epoch 61/100 - Loss: 0.1455\n",
      "Pretraining Epoch 62/100 - Loss: 0.2465\n",
      "Pretraining Epoch 63/100 - Loss: 0.0437\n",
      "Pretraining Epoch 64/100 - Loss: 0.1891\n",
      "Pretraining Epoch 65/100 - Loss: 1.4467\n",
      "Pretraining Epoch 66/100 - Loss: 0.0224\n",
      "Pretraining Epoch 67/100 - Loss: 0.0265\n",
      "Pretraining Epoch 68/100 - Loss: 0.0155\n",
      "Pretraining Epoch 69/100 - Loss: 0.1030\n",
      "Pretraining Epoch 70/100 - Loss: 0.0532\n",
      "Pretraining Epoch 71/100 - Loss: 0.5379\n",
      "Pretraining Epoch 72/100 - Loss: 0.1670\n",
      "Pretraining Epoch 73/100 - Loss: 0.1636\n",
      "Pretraining Epoch 74/100 - Loss: 0.0343\n",
      "Pretraining Epoch 75/100 - Loss: 0.7851\n",
      "Pretraining Epoch 76/100 - Loss: 0.0346\n",
      "Pretraining Epoch 77/100 - Loss: 0.0127\n",
      "Pretraining Epoch 78/100 - Loss: 0.0957\n",
      "Pretraining Epoch 79/100 - Loss: 0.2381\n",
      "Pretraining Epoch 80/100 - Loss: 0.6903\n",
      "Pretraining Epoch 81/100 - Loss: 0.0189\n",
      "Pretraining Epoch 82/100 - Loss: 0.0084\n",
      "Pretraining Epoch 83/100 - Loss: 0.2393\n",
      "Pretraining Epoch 84/100 - Loss: 0.1336\n",
      "Pretraining Epoch 85/100 - Loss: 0.0505\n",
      "Pretraining Epoch 86/100 - Loss: 0.0883\n",
      "Pretraining Epoch 87/100 - Loss: 0.4584\n",
      "Pretraining Epoch 88/100 - Loss: 0.0479\n",
      "Pretraining Epoch 89/100 - Loss: 0.1376\n",
      "Pretraining Epoch 90/100 - Loss: 0.0669\n",
      "Pretraining Epoch 91/100 - Loss: 0.0196\n",
      "Pretraining Epoch 92/100 - Loss: 0.0852\n",
      "Pretraining Epoch 93/100 - Loss: 0.7013\n",
      "Pretraining Epoch 94/100 - Loss: 0.0557\n",
      "Pretraining Epoch 95/100 - Loss: 0.0296\n",
      "Pretraining Epoch 96/100 - Loss: 0.0623\n",
      "Pretraining Epoch 97/100 - Loss: 0.8518\n",
      "Pretraining Epoch 98/100 - Loss: 0.0194\n",
      "Pretraining Epoch 99/100 - Loss: 0.1396\n",
      "Pretraining Epoch 100/100 - Loss: 0.0130\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Pitcher Pretraining....\")\n",
    "pitcher_pretrain_model = train_pretraining_model(\n",
    "    pitcher_pretrain_model, \n",
    "    pitcher_pretrain_loader, \n",
    "    num_epochs=100, \n",
    "    lr=5e-4, \n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "# print(\"\\nStarting Batter Pretraining....\")\n",
    "# batter_pretrain_model = train_pretraining_model(\n",
    "#     batter_pretrain_model, \n",
    "#     batter_pretrain_loader, \n",
    "#     num_epochs=25, \n",
    "#     lr=1e-3, \n",
    "#     device='cuda'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the datasets for each model\n",
    "batter_dataset = MLB_Batter_Dataset(\n",
    "    train_batter_df, \n",
    "    train_pitcher_df, \n",
    "    sequence_length=3, \n",
    "    batter_features=batter_features, \n",
    "    pitcher_features=pitcher_features\n",
    "    )\n",
    "\n",
    "pitcher_dataset = MLB_Pitcher_Dataset(\n",
    "    train_pitcher_df, \n",
    "    train_batter_df, \n",
    "    sequence_length=3, \n",
    "    pitcher_features=pitcher_features, \n",
    "    batter_features=batter_features\n",
    "    )\n",
    "\n",
    "pitcher_dataset_val = MLB_Pitcher_Dataset(\n",
    "    val_pitcher_df, \n",
    "    val_batter_df, \n",
    "    sequence_length=3, \n",
    "    pitcher_features=pitcher_features, \n",
    "    batter_features=batter_features\n",
    "    )\n",
    "\n",
    "#batter_loader = DataLoader(batter_dataset, batch_size=2, shuffle=True)\n",
    "pitcher_loader = DataLoader(pitcher_dataset, batch_size=32, shuffle=True)\n",
    "pitcher_loader_val = DataLoader(pitcher_dataset_val, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Supervised Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batter_model = BatterTransformerModel(\n",
    "    batter_input_dim=len(batter_features),\n",
    "    opp_pitcher_input_dim=len(pitcher_features),\n",
    "    model_dim=64, \n",
    "    n_heads=4, \n",
    "    num_layers=2, \n",
    "    sequence_length=3\n",
    ")\n",
    "    \n",
    "# Load the pretrained encoder weights into the batter model.\n",
    "batter_model.load_pretrained_encoder(batter_pretrain_model)\n",
    "\n",
    "pitcher_model = PitcherTransformerModel(\n",
    "    pitcher_input_dim=len(pitcher_features),\n",
    "    opp_batter_input_dim=len(batter_features),\n",
    "    model_dim=64, \n",
    "    n_heads=4, \n",
    "    num_layers=2, \n",
    "    sequence_length=3,\n",
    "    dropout=0.3\n",
    ")\n",
    "    \n",
    "pitcher_model.load_pretrained_encoder(pitcher_pretrain_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Pitcher Model:\n",
      "Epoch 1/1000 - Train Loss: 7.6310, Val Loss: 7.0077, MAE: 2.1534, RMSE: 2.6472, R2: 0.0089\n",
      "--> Saved best model with val_loss: 7.0077 at epoch 1\n",
      "Epoch 2/1000 - Train Loss: 7.0708, Val Loss: 7.0373, MAE: 2.1669, RMSE: 2.6528, R2: 0.0047\n",
      "Epoch 3/1000 - Train Loss: 6.6798, Val Loss: 6.8552, MAE: 2.1325, RMSE: 2.6182, R2: 0.0304\n",
      "--> Saved best model with val_loss: 6.8552 at epoch 3\n",
      "Epoch 4/1000 - Train Loss: 6.8322, Val Loss: 6.7990, MAE: 2.1132, RMSE: 2.6075, R2: 0.0384\n",
      "--> Saved best model with val_loss: 6.7990 at epoch 4\n",
      "Epoch 5/1000 - Train Loss: 6.6461, Val Loss: 6.7688, MAE: 2.1138, RMSE: 2.6017, R2: 0.0427\n",
      "--> Saved best model with val_loss: 6.7688 at epoch 5\n",
      "Epoch 6/1000 - Train Loss: 6.4847, Val Loss: 6.4714, MAE: 2.0520, RMSE: 2.5439, R2: 0.0847\n",
      "--> Saved best model with val_loss: 6.4714 at epoch 6\n",
      "Epoch 7/1000 - Train Loss: 6.3567, Val Loss: 6.4311, MAE: 2.0456, RMSE: 2.5360, R2: 0.0904\n",
      "--> Saved best model with val_loss: 6.4311 at epoch 7\n",
      "Epoch 8/1000 - Train Loss: 6.3223, Val Loss: 6.4594, MAE: 2.0508, RMSE: 2.5415, R2: 0.0864\n",
      "Epoch 9/1000 - Train Loss: 6.5342, Val Loss: 6.5927, MAE: 2.0889, RMSE: 2.5676, R2: 0.0676\n",
      "Epoch 10/1000 - Train Loss: 6.4661, Val Loss: 6.9831, MAE: 2.1483, RMSE: 2.6426, R2: 0.0123\n",
      "Epoch 11/1000 - Train Loss: 6.4893, Val Loss: 6.4555, MAE: 2.0548, RMSE: 2.5408, R2: 0.0870\n",
      "Epoch 12/1000 - Train Loss: 6.3905, Val Loss: 6.6257, MAE: 2.0941, RMSE: 2.5740, R2: 0.0629\n",
      "Epoch 13/1000 - Train Loss: 6.3674, Val Loss: 6.7044, MAE: 2.0841, RMSE: 2.5893, R2: 0.0518\n",
      "Epoch 14/1000 - Train Loss: 6.3292, Val Loss: 6.4490, MAE: 2.0472, RMSE: 2.5395, R2: 0.0879\n",
      "Epoch 15/1000 - Train Loss: 6.2747, Val Loss: 6.4342, MAE: 2.0425, RMSE: 2.5366, R2: 0.0900\n",
      "Epoch 16/1000 - Train Loss: 6.2665, Val Loss: 6.4816, MAE: 2.0517, RMSE: 2.5459, R2: 0.0833\n",
      "Epoch 17/1000 - Train Loss: 6.2384, Val Loss: 6.4757, MAE: 2.0495, RMSE: 2.5447, R2: 0.0841\n",
      "Epoch 18/1000 - Train Loss: 6.2160, Val Loss: 6.4708, MAE: 2.0481, RMSE: 2.5438, R2: 0.0848\n",
      "Epoch 19/1000 - Train Loss: 6.4491, Val Loss: 6.5342, MAE: 2.0704, RMSE: 2.5562, R2: 0.0758\n",
      "Epoch 20/1000 - Train Loss: 6.4715, Val Loss: 6.6712, MAE: 2.0877, RMSE: 2.5829, R2: 0.0565\n",
      "Epoch 21/1000 - Train Loss: 6.4131, Val Loss: 6.7122, MAE: 2.1021, RMSE: 2.5908, R2: 0.0507\n",
      "Epoch 22/1000 - Train Loss: 6.3567, Val Loss: 6.4530, MAE: 2.0337, RMSE: 2.5403, R2: 0.0873\n",
      "Epoch 23/1000 - Train Loss: 6.3546, Val Loss: 6.8816, MAE: 2.1289, RMSE: 2.6233, R2: 0.0267\n",
      "Epoch 24/1000 - Train Loss: 6.3388, Val Loss: 6.7364, MAE: 2.0913, RMSE: 2.5955, R2: 0.0472\n",
      "Epoch 25/1000 - Train Loss: 6.3717, Val Loss: 6.5715, MAE: 2.0706, RMSE: 2.5635, R2: 0.0706\n",
      "Epoch 26/1000 - Train Loss: 6.2959, Val Loss: 6.4879, MAE: 2.0418, RMSE: 2.5471, R2: 0.0824\n",
      "Epoch 27/1000 - Train Loss: 6.2959, Val Loss: 6.7136, MAE: 2.0686, RMSE: 2.5911, R2: 0.0505\n",
      "Epoch 28/1000 - Train Loss: 6.2679, Val Loss: 6.4861, MAE: 2.0382, RMSE: 2.5468, R2: 0.0826\n",
      "Epoch 29/1000 - Train Loss: 6.2894, Val Loss: 6.4625, MAE: 2.0411, RMSE: 2.5422, R2: 0.0860\n",
      "Epoch 30/1000 - Train Loss: 6.2613, Val Loss: 6.4371, MAE: 2.0414, RMSE: 2.5371, R2: 0.0896\n",
      "Epoch 31/1000 - Train Loss: 6.2319, Val Loss: 6.5091, MAE: 2.0326, RMSE: 2.5513, R2: 0.0794\n",
      "Epoch 32/1000 - Train Loss: 6.2155, Val Loss: 6.4974, MAE: 2.0464, RMSE: 2.5490, R2: 0.0810\n",
      "Epoch 33/1000 - Train Loss: 6.2092, Val Loss: 6.4515, MAE: 2.0425, RMSE: 2.5400, R2: 0.0875\n",
      "Epoch 34/1000 - Train Loss: 6.1958, Val Loss: 6.4289, MAE: 2.0285, RMSE: 2.5355, R2: 0.0907\n",
      "--> Saved best model with val_loss: 6.4289 at epoch 34\n",
      "Epoch 35/1000 - Train Loss: 6.1757, Val Loss: 6.4353, MAE: 2.0345, RMSE: 2.5368, R2: 0.0898\n",
      "Epoch 36/1000 - Train Loss: 6.1854, Val Loss: 6.4287, MAE: 2.0287, RMSE: 2.5355, R2: 0.0908\n",
      "--> Saved best model with val_loss: 6.4287 at epoch 36\n",
      "Epoch 37/1000 - Train Loss: 6.1775, Val Loss: 6.4346, MAE: 2.0286, RMSE: 2.5366, R2: 0.0899\n",
      "Epoch 38/1000 - Train Loss: 6.1667, Val Loss: 6.4349, MAE: 2.0287, RMSE: 2.5367, R2: 0.0899\n",
      "Epoch 39/1000 - Train Loss: 6.3352, Val Loss: 6.7116, MAE: 2.0904, RMSE: 2.5907, R2: 0.0507\n",
      "Epoch 40/1000 - Train Loss: 6.2999, Val Loss: 6.6022, MAE: 2.0751, RMSE: 2.5695, R2: 0.0662\n",
      "Epoch 41/1000 - Train Loss: 6.3291, Val Loss: 6.4414, MAE: 2.0415, RMSE: 2.5380, R2: 0.0890\n",
      "Epoch 42/1000 - Train Loss: 6.3812, Val Loss: 6.4293, MAE: 2.0396, RMSE: 2.5356, R2: 0.0907\n",
      "Epoch 43/1000 - Train Loss: 6.2813, Val Loss: 6.4436, MAE: 2.0490, RMSE: 2.5384, R2: 0.0887\n",
      "Epoch 44/1000 - Train Loss: 6.2922, Val Loss: 6.5447, MAE: 2.0649, RMSE: 2.5583, R2: 0.0744\n",
      "Epoch 45/1000 - Train Loss: 6.3228, Val Loss: 6.5383, MAE: 2.0736, RMSE: 2.5570, R2: 0.0753\n",
      "Epoch 46/1000 - Train Loss: 6.2691, Val Loss: 6.5099, MAE: 2.0493, RMSE: 2.5514, R2: 0.0793\n",
      "Epoch 47/1000 - Train Loss: 6.2929, Val Loss: 6.4497, MAE: 2.0471, RMSE: 2.5396, R2: 0.0878\n",
      "Epoch 48/1000 - Train Loss: 6.2548, Val Loss: 7.0896, MAE: 2.1573, RMSE: 2.6626, R2: -0.0027\n",
      "Epoch 49/1000 - Train Loss: 6.3009, Val Loss: 6.4883, MAE: 2.0421, RMSE: 2.5472, R2: 0.0823\n",
      "Epoch 50/1000 - Train Loss: 6.2935, Val Loss: 6.5434, MAE: 2.0619, RMSE: 2.5580, R2: 0.0745\n",
      "Epoch 51/1000 - Train Loss: 6.2872, Val Loss: 6.4430, MAE: 2.0304, RMSE: 2.5383, R2: 0.0887\n",
      "Epoch 52/1000 - Train Loss: 6.3039, Val Loss: 6.4561, MAE: 2.0355, RMSE: 2.5409, R2: 0.0869\n",
      "Epoch 53/1000 - Train Loss: 6.2848, Val Loss: 6.4759, MAE: 2.0375, RMSE: 2.5448, R2: 0.0841\n",
      "Epoch 54/1000 - Train Loss: 6.2528, Val Loss: 6.5860, MAE: 2.0663, RMSE: 2.5663, R2: 0.0685\n",
      "Epoch 55/1000 - Train Loss: 6.2763, Val Loss: 6.4874, MAE: 2.0462, RMSE: 2.5470, R2: 0.0825\n",
      "Epoch 56/1000 - Train Loss: 6.2515, Val Loss: 6.4728, MAE: 2.0386, RMSE: 2.5442, R2: 0.0845\n",
      "Epoch 57/1000 - Train Loss: 6.2117, Val Loss: 6.4428, MAE: 2.0369, RMSE: 2.5383, R2: 0.0888\n",
      "Epoch 58/1000 - Train Loss: 6.2398, Val Loss: 6.4538, MAE: 2.0417, RMSE: 2.5404, R2: 0.0872\n",
      "Epoch 59/1000 - Train Loss: 6.1780, Val Loss: 6.7243, MAE: 2.0830, RMSE: 2.5931, R2: 0.0489\n",
      "Epoch 60/1000 - Train Loss: 6.2308, Val Loss: 6.4716, MAE: 2.0388, RMSE: 2.5439, R2: 0.0847\n",
      "Epoch 61/1000 - Train Loss: 6.2012, Val Loss: 6.5377, MAE: 2.0632, RMSE: 2.5569, R2: 0.0753\n",
      "Epoch 62/1000 - Train Loss: 6.1815, Val Loss: 6.4585, MAE: 2.0356, RMSE: 2.5414, R2: 0.0865\n",
      "Epoch 63/1000 - Train Loss: 6.1780, Val Loss: 6.4428, MAE: 2.0343, RMSE: 2.5383, R2: 0.0888\n",
      "Epoch 64/1000 - Train Loss: 6.1281, Val Loss: 6.5440, MAE: 2.0543, RMSE: 2.5581, R2: 0.0745\n",
      "Epoch 65/1000 - Train Loss: 6.1602, Val Loss: 6.4678, MAE: 2.0465, RMSE: 2.5432, R2: 0.0852\n",
      "Epoch 66/1000 - Train Loss: 6.1231, Val Loss: 6.4949, MAE: 2.0448, RMSE: 2.5485, R2: 0.0814\n",
      "Epoch 67/1000 - Train Loss: 6.1272, Val Loss: 6.4450, MAE: 2.0344, RMSE: 2.5387, R2: 0.0885\n",
      "Epoch 68/1000 - Train Loss: 6.1140, Val Loss: 6.4363, MAE: 2.0316, RMSE: 2.5370, R2: 0.0897\n",
      "Epoch 69/1000 - Train Loss: 6.1128, Val Loss: 6.4475, MAE: 2.0323, RMSE: 2.5392, R2: 0.0881\n",
      "Epoch 70/1000 - Train Loss: 6.0968, Val Loss: 6.4451, MAE: 2.0334, RMSE: 2.5387, R2: 0.0884\n",
      "Epoch 71/1000 - Train Loss: 6.1169, Val Loss: 6.4493, MAE: 2.0329, RMSE: 2.5395, R2: 0.0878\n",
      "Epoch 72/1000 - Train Loss: 6.0762, Val Loss: 6.4670, MAE: 2.0330, RMSE: 2.5430, R2: 0.0853\n",
      "Epoch 73/1000 - Train Loss: 6.0881, Val Loss: 6.4511, MAE: 2.0332, RMSE: 2.5399, R2: 0.0876\n",
      "Epoch 74/1000 - Train Loss: 6.0628, Val Loss: 6.4677, MAE: 2.0360, RMSE: 2.5432, R2: 0.0852\n",
      "Epoch 75/1000 - Train Loss: 6.0812, Val Loss: 6.4618, MAE: 2.0349, RMSE: 2.5420, R2: 0.0861\n",
      "Epoch 76/1000 - Train Loss: 6.0687, Val Loss: 6.4614, MAE: 2.0345, RMSE: 2.5419, R2: 0.0861\n",
      "Epoch 77/1000 - Train Loss: 6.0772, Val Loss: 6.4616, MAE: 2.0345, RMSE: 2.5420, R2: 0.0861\n",
      "Epoch 78/1000 - Train Loss: 6.0639, Val Loss: 6.4615, MAE: 2.0344, RMSE: 2.5419, R2: 0.0861\n",
      "Epoch 79/1000 - Train Loss: 6.2756, Val Loss: 6.4120, MAE: 2.0377, RMSE: 2.5322, R2: 0.0931\n",
      "--> Saved best model with val_loss: 6.4120 at epoch 79\n",
      "Epoch 80/1000 - Train Loss: 6.2963, Val Loss: 6.7042, MAE: 2.0908, RMSE: 2.5893, R2: 0.0518\n",
      "Epoch 81/1000 - Train Loss: 6.2407, Val Loss: 6.4646, MAE: 2.0363, RMSE: 2.5426, R2: 0.0857\n",
      "Epoch 82/1000 - Train Loss: 6.2261, Val Loss: 6.4939, MAE: 2.0342, RMSE: 2.5483, R2: 0.0815\n",
      "Epoch 83/1000 - Train Loss: 6.2823, Val Loss: 6.5344, MAE: 2.0506, RMSE: 2.5562, R2: 0.0758\n",
      "Epoch 84/1000 - Train Loss: 6.2252, Val Loss: 6.7733, MAE: 2.0806, RMSE: 2.6026, R2: 0.0420\n",
      "Epoch 85/1000 - Train Loss: 6.2565, Val Loss: 6.7639, MAE: 2.0901, RMSE: 2.6008, R2: 0.0433\n",
      "Epoch 86/1000 - Train Loss: 6.2499, Val Loss: 6.4284, MAE: 2.0433, RMSE: 2.5354, R2: 0.0908\n",
      "Epoch 87/1000 - Train Loss: 6.2197, Val Loss: 6.4727, MAE: 2.0346, RMSE: 2.5442, R2: 0.0845\n",
      "Epoch 88/1000 - Train Loss: 6.2368, Val Loss: 6.4523, MAE: 2.0477, RMSE: 2.5401, R2: 0.0874\n",
      "Epoch 89/1000 - Train Loss: 6.2761, Val Loss: 6.8691, MAE: 2.1227, RMSE: 2.6209, R2: 0.0285\n",
      "Epoch 90/1000 - Train Loss: 6.1995, Val Loss: 6.4432, MAE: 2.0369, RMSE: 2.5383, R2: 0.0887\n",
      "Epoch 91/1000 - Train Loss: 6.2221, Val Loss: 6.5050, MAE: 2.0556, RMSE: 2.5505, R2: 0.0800\n",
      "Epoch 92/1000 - Train Loss: 6.2676, Val Loss: 6.4148, MAE: 2.0413, RMSE: 2.5327, R2: 0.0927\n",
      "Epoch 93/1000 - Train Loss: 6.2211, Val Loss: 6.6124, MAE: 2.0664, RMSE: 2.5715, R2: 0.0648\n",
      "Epoch 94/1000 - Train Loss: 6.2009, Val Loss: 6.4024, MAE: 2.0401, RMSE: 2.5303, R2: 0.0945\n",
      "--> Saved best model with val_loss: 6.4024 at epoch 94\n",
      "Epoch 95/1000 - Train Loss: 6.2519, Val Loss: 6.5195, MAE: 2.0714, RMSE: 2.5533, R2: 0.0779\n",
      "Epoch 96/1000 - Train Loss: 6.2193, Val Loss: 6.5038, MAE: 2.0482, RMSE: 2.5502, R2: 0.0801\n",
      "Epoch 97/1000 - Train Loss: 6.1908, Val Loss: 6.5124, MAE: 2.0409, RMSE: 2.5519, R2: 0.0789\n",
      "Epoch 98/1000 - Train Loss: 6.1426, Val Loss: 6.4896, MAE: 2.0438, RMSE: 2.5475, R2: 0.0822\n",
      "Epoch 99/1000 - Train Loss: 6.1678, Val Loss: 6.4571, MAE: 2.0277, RMSE: 2.5411, R2: 0.0867\n",
      "Epoch 100/1000 - Train Loss: 6.1677, Val Loss: 6.4344, MAE: 2.0400, RMSE: 2.5366, R2: 0.0899\n",
      "Epoch 101/1000 - Train Loss: 6.1758, Val Loss: 6.7911, MAE: 2.0884, RMSE: 2.6060, R2: 0.0395\n",
      "Epoch 102/1000 - Train Loss: 6.1769, Val Loss: 6.5637, MAE: 2.0698, RMSE: 2.5620, R2: 0.0717\n",
      "Epoch 103/1000 - Train Loss: 6.1748, Val Loss: 6.4841, MAE: 2.0499, RMSE: 2.5464, R2: 0.0829\n",
      "Epoch 104/1000 - Train Loss: 6.1681, Val Loss: 6.5091, MAE: 2.0424, RMSE: 2.5513, R2: 0.0794\n",
      "Epoch 105/1000 - Train Loss: 6.1837, Val Loss: 6.5635, MAE: 2.0678, RMSE: 2.5619, R2: 0.0717\n",
      "Epoch 106/1000 - Train Loss: 6.1608, Val Loss: 6.5223, MAE: 2.0517, RMSE: 2.5539, R2: 0.0775\n",
      "Epoch 107/1000 - Train Loss: 6.1480, Val Loss: 6.6327, MAE: 2.0835, RMSE: 2.5754, R2: 0.0619\n",
      "Epoch 108/1000 - Train Loss: 6.1062, Val Loss: 6.4959, MAE: 2.0516, RMSE: 2.5487, R2: 0.0813\n",
      "Epoch 109/1000 - Train Loss: 6.1481, Val Loss: 6.5528, MAE: 2.0528, RMSE: 2.5598, R2: 0.0732\n",
      "Epoch 110/1000 - Train Loss: 6.1052, Val Loss: 6.5138, MAE: 2.0491, RMSE: 2.5522, R2: 0.0787\n",
      "Epoch 111/1000 - Train Loss: 6.1091, Val Loss: 6.5338, MAE: 2.0542, RMSE: 2.5561, R2: 0.0759\n",
      "Epoch 112/1000 - Train Loss: 6.0586, Val Loss: 6.5126, MAE: 2.0595, RMSE: 2.5520, R2: 0.0789\n",
      "Epoch 113/1000 - Train Loss: 6.0899, Val Loss: 6.5882, MAE: 2.0614, RMSE: 2.5667, R2: 0.0682\n",
      "Epoch 114/1000 - Train Loss: 6.1142, Val Loss: 6.5099, MAE: 2.0483, RMSE: 2.5515, R2: 0.0793\n",
      "Epoch 115/1000 - Train Loss: 6.0898, Val Loss: 6.4740, MAE: 2.0439, RMSE: 2.5444, R2: 0.0843\n",
      "Epoch 116/1000 - Train Loss: 6.0641, Val Loss: 6.5737, MAE: 2.0673, RMSE: 2.5639, R2: 0.0702\n",
      "Epoch 117/1000 - Train Loss: 6.0601, Val Loss: 6.5814, MAE: 2.0582, RMSE: 2.5654, R2: 0.0692\n",
      "Epoch 118/1000 - Train Loss: 6.0723, Val Loss: 6.4114, MAE: 2.0321, RMSE: 2.5321, R2: 0.0932\n",
      "Epoch 119/1000 - Train Loss: 6.0391, Val Loss: 6.4421, MAE: 2.0332, RMSE: 2.5381, R2: 0.0889\n",
      "Epoch 120/1000 - Train Loss: 6.0397, Val Loss: 6.4659, MAE: 2.0405, RMSE: 2.5428, R2: 0.0855\n",
      "Epoch 121/1000 - Train Loss: 6.0421, Val Loss: 6.5444, MAE: 2.0528, RMSE: 2.5582, R2: 0.0744\n",
      "Epoch 122/1000 - Train Loss: 6.0154, Val Loss: 6.5270, MAE: 2.0399, RMSE: 2.5548, R2: 0.0769\n",
      "Epoch 123/1000 - Train Loss: 6.0525, Val Loss: 6.4285, MAE: 2.0375, RMSE: 2.5354, R2: 0.0908\n",
      "Epoch 124/1000 - Train Loss: 6.0234, Val Loss: 6.5127, MAE: 2.0421, RMSE: 2.5520, R2: 0.0789\n",
      "Epoch 125/1000 - Train Loss: 5.9926, Val Loss: 6.4566, MAE: 2.0369, RMSE: 2.5410, R2: 0.0868\n",
      "Epoch 126/1000 - Train Loss: 6.0138, Val Loss: 6.4681, MAE: 2.0379, RMSE: 2.5432, R2: 0.0852\n",
      "Epoch 127/1000 - Train Loss: 5.9942, Val Loss: 6.4827, MAE: 2.0425, RMSE: 2.5461, R2: 0.0831\n",
      "Epoch 128/1000 - Train Loss: 5.9767, Val Loss: 6.4925, MAE: 2.0367, RMSE: 2.5480, R2: 0.0817\n",
      "Epoch 129/1000 - Train Loss: 5.9564, Val Loss: 6.5016, MAE: 2.0387, RMSE: 2.5498, R2: 0.0804\n",
      "Epoch 130/1000 - Train Loss: 5.9276, Val Loss: 6.4692, MAE: 2.0375, RMSE: 2.5435, R2: 0.0850\n",
      "Epoch 131/1000 - Train Loss: 5.9502, Val Loss: 6.4627, MAE: 2.0399, RMSE: 2.5422, R2: 0.0860\n",
      "Epoch 132/1000 - Train Loss: 5.9102, Val Loss: 6.4879, MAE: 2.0396, RMSE: 2.5471, R2: 0.0824\n",
      "Epoch 133/1000 - Train Loss: 5.9292, Val Loss: 6.5586, MAE: 2.0466, RMSE: 2.5610, R2: 0.0724\n",
      "Epoch 134/1000 - Train Loss: 5.9166, Val Loss: 6.5010, MAE: 2.0428, RMSE: 2.5497, R2: 0.0805\n",
      "Epoch 135/1000 - Train Loss: 5.9309, Val Loss: 6.4373, MAE: 2.0370, RMSE: 2.5372, R2: 0.0895\n",
      "Epoch 136/1000 - Train Loss: 5.9207, Val Loss: 6.4998, MAE: 2.0440, RMSE: 2.5495, R2: 0.0807\n",
      "Epoch 137/1000 - Train Loss: 5.8951, Val Loss: 6.4820, MAE: 2.0397, RMSE: 2.5460, R2: 0.0832\n",
      "Epoch 138/1000 - Train Loss: 5.8905, Val Loss: 6.5014, MAE: 2.0407, RMSE: 2.5498, R2: 0.0805\n",
      "Epoch 139/1000 - Train Loss: 5.8899, Val Loss: 6.4694, MAE: 2.0372, RMSE: 2.5435, R2: 0.0850\n",
      "Epoch 140/1000 - Train Loss: 5.8923, Val Loss: 6.5596, MAE: 2.0470, RMSE: 2.5612, R2: 0.0722\n",
      "Epoch 141/1000 - Train Loss: 5.8888, Val Loss: 6.5035, MAE: 2.0416, RMSE: 2.5502, R2: 0.0802\n",
      "Epoch 142/1000 - Train Loss: 5.8916, Val Loss: 6.4915, MAE: 2.0394, RMSE: 2.5479, R2: 0.0819\n",
      "Epoch 143/1000 - Train Loss: 5.8594, Val Loss: 6.4875, MAE: 2.0379, RMSE: 2.5471, R2: 0.0824\n",
      "Epoch 144/1000 - Train Loss: 5.8333, Val Loss: 6.4837, MAE: 2.0386, RMSE: 2.5463, R2: 0.0830\n",
      "Epoch 145/1000 - Train Loss: 5.8503, Val Loss: 6.4971, MAE: 2.0397, RMSE: 2.5489, R2: 0.0811\n",
      "Epoch 146/1000 - Train Loss: 5.8285, Val Loss: 6.4860, MAE: 2.0384, RMSE: 2.5468, R2: 0.0827\n",
      "Epoch 147/1000 - Train Loss: 5.8252, Val Loss: 6.5224, MAE: 2.0433, RMSE: 2.5539, R2: 0.0775\n",
      "Epoch 148/1000 - Train Loss: 5.8429, Val Loss: 6.4932, MAE: 2.0398, RMSE: 2.5482, R2: 0.0816\n",
      "Epoch 149/1000 - Train Loss: 5.8513, Val Loss: 6.4928, MAE: 2.0397, RMSE: 2.5481, R2: 0.0817\n",
      "Epoch 150/1000 - Train Loss: 5.8274, Val Loss: 6.4910, MAE: 2.0391, RMSE: 2.5477, R2: 0.0819\n",
      "Epoch 151/1000 - Train Loss: 5.8486, Val Loss: 6.4980, MAE: 2.0399, RMSE: 2.5491, R2: 0.0810\n",
      "Epoch 152/1000 - Train Loss: 5.8252, Val Loss: 6.4978, MAE: 2.0395, RMSE: 2.5491, R2: 0.0810\n",
      "Epoch 153/1000 - Train Loss: 5.8259, Val Loss: 6.4957, MAE: 2.0395, RMSE: 2.5487, R2: 0.0813\n",
      "Epoch 154/1000 - Train Loss: 5.8378, Val Loss: 6.4950, MAE: 2.0394, RMSE: 2.5485, R2: 0.0814\n",
      "Epoch 155/1000 - Train Loss: 5.8228, Val Loss: 6.4947, MAE: 2.0393, RMSE: 2.5485, R2: 0.0814\n",
      "Epoch 156/1000 - Train Loss: 5.8332, Val Loss: 6.4963, MAE: 2.0395, RMSE: 2.5488, R2: 0.0812\n",
      "Epoch 157/1000 - Train Loss: 5.8047, Val Loss: 6.4960, MAE: 2.0395, RMSE: 2.5487, R2: 0.0812\n",
      "Epoch 158/1000 - Train Loss: 5.8197, Val Loss: 6.4961, MAE: 2.0395, RMSE: 2.5487, R2: 0.0812\n",
      "Epoch 159/1000 - Train Loss: 6.0249, Val Loss: 6.4213, MAE: 2.0330, RMSE: 2.5340, R2: 0.0918\n",
      "Epoch 160/1000 - Train Loss: 6.0549, Val Loss: 6.4240, MAE: 2.0364, RMSE: 2.5346, R2: 0.0914\n",
      "Epoch 161/1000 - Train Loss: 6.1786, Val Loss: 6.6216, MAE: 2.0788, RMSE: 2.5732, R2: 0.0635\n",
      "Epoch 162/1000 - Train Loss: 6.1862, Val Loss: 6.6192, MAE: 2.0771, RMSE: 2.5728, R2: 0.0638\n",
      "Epoch 163/1000 - Train Loss: 6.1734, Val Loss: 6.5267, MAE: 2.0536, RMSE: 2.5547, R2: 0.0769\n",
      "Epoch 164/1000 - Train Loss: 6.1265, Val Loss: 6.7854, MAE: 2.0834, RMSE: 2.6049, R2: 0.0403\n",
      "Epoch 165/1000 - Train Loss: 6.0858, Val Loss: 6.8571, MAE: 2.0981, RMSE: 2.6186, R2: 0.0302\n",
      "Epoch 166/1000 - Train Loss: 6.1271, Val Loss: 6.6139, MAE: 2.0643, RMSE: 2.5717, R2: 0.0646\n",
      "Epoch 167/1000 - Train Loss: 6.0998, Val Loss: 6.6067, MAE: 2.0685, RMSE: 2.5703, R2: 0.0656\n",
      "Epoch 168/1000 - Train Loss: 6.0600, Val Loss: 6.6567, MAE: 2.0775, RMSE: 2.5801, R2: 0.0585\n",
      "Epoch 169/1000 - Train Loss: 6.0428, Val Loss: 6.8437, MAE: 2.0941, RMSE: 2.6161, R2: 0.0321\n",
      "Epoch 170/1000 - Train Loss: 6.0580, Val Loss: 6.4838, MAE: 2.0533, RMSE: 2.5463, R2: 0.0830\n",
      "Epoch 171/1000 - Train Loss: 6.0759, Val Loss: 6.6303, MAE: 2.0605, RMSE: 2.5749, R2: 0.0622\n",
      "Epoch 172/1000 - Train Loss: 6.0870, Val Loss: 6.5602, MAE: 2.0665, RMSE: 2.5613, R2: 0.0722\n",
      "Epoch 173/1000 - Train Loss: 6.0162, Val Loss: 6.4415, MAE: 2.0413, RMSE: 2.5380, R2: 0.0890\n",
      "Epoch 174/1000 - Train Loss: 6.0096, Val Loss: 6.6025, MAE: 2.0543, RMSE: 2.5695, R2: 0.0662\n",
      "Epoch 175/1000 - Train Loss: 6.0480, Val Loss: 6.5261, MAE: 2.0480, RMSE: 2.5546, R2: 0.0770\n",
      "Epoch 176/1000 - Train Loss: 6.0250, Val Loss: 6.5463, MAE: 2.0554, RMSE: 2.5586, R2: 0.0741\n",
      "Epoch 177/1000 - Train Loss: 6.0075, Val Loss: 6.4574, MAE: 2.0413, RMSE: 2.5411, R2: 0.0867\n",
      "Epoch 178/1000 - Train Loss: 6.0632, Val Loss: 6.5294, MAE: 2.0469, RMSE: 2.5553, R2: 0.0765\n",
      "Epoch 179/1000 - Train Loss: 5.9857, Val Loss: 6.6704, MAE: 2.0666, RMSE: 2.5827, R2: 0.0566\n",
      "Epoch 180/1000 - Train Loss: 6.0066, Val Loss: 6.4077, MAE: 2.0374, RMSE: 2.5313, R2: 0.0937\n",
      "Epoch 181/1000 - Train Loss: 6.0416, Val Loss: 6.5357, MAE: 2.0435, RMSE: 2.5565, R2: 0.0756\n",
      "Epoch 182/1000 - Train Loss: 5.9990, Val Loss: 6.6261, MAE: 2.0639, RMSE: 2.5741, R2: 0.0628\n",
      "Epoch 183/1000 - Train Loss: 6.0175, Val Loss: 6.6553, MAE: 2.0847, RMSE: 2.5798, R2: 0.0587\n",
      "Epoch 184/1000 - Train Loss: 5.9690, Val Loss: 6.5125, MAE: 2.0490, RMSE: 2.5520, R2: 0.0789\n",
      "Epoch 185/1000 - Train Loss: 6.0249, Val Loss: 6.5689, MAE: 2.0744, RMSE: 2.5630, R2: 0.0709\n",
      "Epoch 186/1000 - Train Loss: 6.0286, Val Loss: 6.4782, MAE: 2.0497, RMSE: 2.5452, R2: 0.0838\n",
      "Epoch 187/1000 - Train Loss: 5.9321, Val Loss: 6.4485, MAE: 2.0466, RMSE: 2.5394, R2: 0.0880\n",
      "Epoch 188/1000 - Train Loss: 5.9511, Val Loss: 6.4749, MAE: 2.0473, RMSE: 2.5446, R2: 0.0842\n",
      "Epoch 189/1000 - Train Loss: 5.9333, Val Loss: 6.5823, MAE: 2.0649, RMSE: 2.5656, R2: 0.0690\n",
      "Epoch 190/1000 - Train Loss: 5.9710, Val Loss: 6.5019, MAE: 2.0507, RMSE: 2.5499, R2: 0.0804\n",
      "Epoch 191/1000 - Train Loss: 5.9306, Val Loss: 6.5680, MAE: 2.0638, RMSE: 2.5628, R2: 0.0711\n",
      "Epoch 192/1000 - Train Loss: 5.9082, Val Loss: 6.4844, MAE: 2.0525, RMSE: 2.5464, R2: 0.0829\n",
      "Epoch 193/1000 - Train Loss: 5.9199, Val Loss: 6.7381, MAE: 2.0764, RMSE: 2.5958, R2: 0.0470\n",
      "Epoch 194/1000 - Train Loss: 5.9040, Val Loss: 6.8307, MAE: 2.0901, RMSE: 2.6136, R2: 0.0339\n",
      "Epoch 195/1000 - Train Loss: 5.9060, Val Loss: 6.5502, MAE: 2.0532, RMSE: 2.5593, R2: 0.0736\n",
      "Epoch 196/1000 - Train Loss: 5.9167, Val Loss: 6.5085, MAE: 2.0518, RMSE: 2.5512, R2: 0.0795\n",
      "Epoch 197/1000 - Train Loss: 5.8884, Val Loss: 6.4689, MAE: 2.0465, RMSE: 2.5434, R2: 0.0851\n",
      "Epoch 198/1000 - Train Loss: 5.9437, Val Loss: 6.5379, MAE: 2.0550, RMSE: 2.5569, R2: 0.0753\n",
      "Epoch 199/1000 - Train Loss: 5.9123, Val Loss: 6.5195, MAE: 2.0527, RMSE: 2.5533, R2: 0.0779\n",
      "Epoch 200/1000 - Train Loss: 5.8582, Val Loss: 6.5856, MAE: 2.0558, RMSE: 2.5662, R2: 0.0686\n",
      "Epoch 201/1000 - Train Loss: 5.8558, Val Loss: 6.6514, MAE: 2.0709, RMSE: 2.5790, R2: 0.0593\n",
      "Epoch 202/1000 - Train Loss: 5.8168, Val Loss: 6.7078, MAE: 2.0664, RMSE: 2.5899, R2: 0.0513\n",
      "Epoch 203/1000 - Train Loss: 5.8208, Val Loss: 6.6665, MAE: 2.0657, RMSE: 2.5820, R2: 0.0571\n",
      "Epoch 204/1000 - Train Loss: 5.8805, Val Loss: 6.6717, MAE: 2.0739, RMSE: 2.5830, R2: 0.0564\n",
      "Epoch 205/1000 - Train Loss: 5.8260, Val Loss: 6.6356, MAE: 2.0720, RMSE: 2.5760, R2: 0.0615\n",
      "Epoch 206/1000 - Train Loss: 5.8748, Val Loss: 6.8423, MAE: 2.0911, RMSE: 2.6158, R2: 0.0323\n",
      "Epoch 207/1000 - Train Loss: 5.7775, Val Loss: 6.8617, MAE: 2.1053, RMSE: 2.6195, R2: 0.0295\n",
      "Epoch 208/1000 - Train Loss: 5.7629, Val Loss: 6.7749, MAE: 2.0973, RMSE: 2.6029, R2: 0.0418\n",
      "Epoch 209/1000 - Train Loss: 5.8180, Val Loss: 6.6720, MAE: 2.0833, RMSE: 2.5830, R2: 0.0563\n",
      "Epoch 210/1000 - Train Loss: 5.7613, Val Loss: 6.6486, MAE: 2.0756, RMSE: 2.5785, R2: 0.0597\n",
      "Epoch 211/1000 - Train Loss: 5.7378, Val Loss: 6.6435, MAE: 2.0780, RMSE: 2.5775, R2: 0.0604\n",
      "Epoch 212/1000 - Train Loss: 5.6873, Val Loss: 6.6495, MAE: 2.0695, RMSE: 2.5787, R2: 0.0595\n",
      "Epoch 213/1000 - Train Loss: 5.7215, Val Loss: 6.6286, MAE: 2.0722, RMSE: 2.5746, R2: 0.0625\n",
      "Epoch 214/1000 - Train Loss: 5.7304, Val Loss: 6.5566, MAE: 2.0730, RMSE: 2.5606, R2: 0.0727\n",
      "Epoch 215/1000 - Train Loss: 5.6958, Val Loss: 6.7840, MAE: 2.0938, RMSE: 2.6046, R2: 0.0405\n",
      "Epoch 216/1000 - Train Loss: 5.6325, Val Loss: 6.6235, MAE: 2.0725, RMSE: 2.5736, R2: 0.0632\n",
      "Epoch 217/1000 - Train Loss: 5.6397, Val Loss: 6.6863, MAE: 2.0843, RMSE: 2.5858, R2: 0.0543\n",
      "Epoch 218/1000 - Train Loss: 5.6303, Val Loss: 6.8058, MAE: 2.0819, RMSE: 2.6088, R2: 0.0374\n",
      "Epoch 219/1000 - Train Loss: 5.6420, Val Loss: 6.8041, MAE: 2.0900, RMSE: 2.6085, R2: 0.0377\n",
      "Epoch 220/1000 - Train Loss: 5.6267, Val Loss: 6.6821, MAE: 2.0881, RMSE: 2.5850, R2: 0.0549\n",
      "Epoch 221/1000 - Train Loss: 5.6191, Val Loss: 6.8886, MAE: 2.1061, RMSE: 2.6246, R2: 0.0257\n",
      "Epoch 222/1000 - Train Loss: 5.5540, Val Loss: 6.5922, MAE: 2.0695, RMSE: 2.5675, R2: 0.0676\n",
      "Epoch 223/1000 - Train Loss: 5.5406, Val Loss: 6.7461, MAE: 2.0981, RMSE: 2.5973, R2: 0.0459\n",
      "Epoch 224/1000 - Train Loss: 5.5689, Val Loss: 6.8481, MAE: 2.1066, RMSE: 2.6169, R2: 0.0314\n",
      "Epoch 225/1000 - Train Loss: 5.5400, Val Loss: 6.7621, MAE: 2.0958, RMSE: 2.6004, R2: 0.0436\n",
      "Epoch 226/1000 - Train Loss: 5.5162, Val Loss: 6.7384, MAE: 2.0962, RMSE: 2.5959, R2: 0.0470\n",
      "Epoch 227/1000 - Train Loss: 5.4858, Val Loss: 6.6628, MAE: 2.0805, RMSE: 2.5812, R2: 0.0576\n",
      "Epoch 228/1000 - Train Loss: 5.4662, Val Loss: 6.7303, MAE: 2.0901, RMSE: 2.5943, R2: 0.0481\n",
      "Epoch 229/1000 - Train Loss: 5.4764, Val Loss: 6.8007, MAE: 2.1055, RMSE: 2.6078, R2: 0.0381\n",
      "Epoch 230/1000 - Train Loss: 5.4841, Val Loss: 6.8990, MAE: 2.1180, RMSE: 2.6266, R2: 0.0242\n",
      "Epoch 231/1000 - Train Loss: 5.4353, Val Loss: 6.7518, MAE: 2.1031, RMSE: 2.5984, R2: 0.0451\n",
      "Epoch 232/1000 - Train Loss: 5.4024, Val Loss: 6.9904, MAE: 2.1402, RMSE: 2.6439, R2: 0.0113\n",
      "Epoch 233/1000 - Train Loss: 5.3858, Val Loss: 7.1629, MAE: 2.1592, RMSE: 2.6764, R2: -0.0131\n",
      "Epoch 234/1000 - Train Loss: 5.3491, Val Loss: 6.9316, MAE: 2.1275, RMSE: 2.6328, R2: 0.0196\n",
      "Epoch 235/1000 - Train Loss: 5.3611, Val Loss: 6.7729, MAE: 2.0980, RMSE: 2.6025, R2: 0.0421\n",
      "Epoch 236/1000 - Train Loss: 5.3992, Val Loss: 6.9888, MAE: 2.1326, RMSE: 2.6436, R2: 0.0115\n",
      "Epoch 237/1000 - Train Loss: 5.3040, Val Loss: 7.1878, MAE: 2.1602, RMSE: 2.6810, R2: -0.0166\n",
      "Epoch 238/1000 - Train Loss: 5.2947, Val Loss: 6.9492, MAE: 2.1247, RMSE: 2.6361, R2: 0.0171\n",
      "Epoch 239/1000 - Train Loss: 5.3122, Val Loss: 6.9632, MAE: 2.1235, RMSE: 2.6388, R2: 0.0152\n",
      "Epoch 240/1000 - Train Loss: 5.2662, Val Loss: 7.0816, MAE: 2.1448, RMSE: 2.6611, R2: -0.0016\n",
      "Epoch 241/1000 - Train Loss: 5.2723, Val Loss: 7.1000, MAE: 2.1477, RMSE: 2.6646, R2: -0.0042\n",
      "Epoch 242/1000 - Train Loss: 5.1808, Val Loss: 7.2518, MAE: 2.1712, RMSE: 2.6929, R2: -0.0257\n",
      "Epoch 243/1000 - Train Loss: 5.1959, Val Loss: 7.0291, MAE: 2.1353, RMSE: 2.6512, R2: 0.0058\n",
      "Epoch 244/1000 - Train Loss: 5.1351, Val Loss: 7.0074, MAE: 2.1385, RMSE: 2.6472, R2: 0.0089\n",
      "Epoch 245/1000 - Train Loss: 5.1418, Val Loss: 7.0768, MAE: 2.1491, RMSE: 2.6602, R2: -0.0009\n",
      "Epoch 246/1000 - Train Loss: 5.1014, Val Loss: 7.1689, MAE: 2.1667, RMSE: 2.6775, R2: -0.0139\n",
      "Epoch 247/1000 - Train Loss: 5.1018, Val Loss: 7.2596, MAE: 2.1678, RMSE: 2.6944, R2: -0.0268\n",
      "Epoch 248/1000 - Train Loss: 5.1260, Val Loss: 7.2041, MAE: 2.1614, RMSE: 2.6840, R2: -0.0189\n",
      "Epoch 249/1000 - Train Loss: 5.0424, Val Loss: 7.3670, MAE: 2.1849, RMSE: 2.7142, R2: -0.0420\n",
      "Epoch 250/1000 - Train Loss: 5.0013, Val Loss: 7.0496, MAE: 2.1429, RMSE: 2.6551, R2: 0.0029\n",
      "Epoch 251/1000 - Train Loss: 5.0303, Val Loss: 7.5026, MAE: 2.2106, RMSE: 2.7391, R2: -0.0611\n",
      "Epoch 252/1000 - Train Loss: 5.0082, Val Loss: 7.3433, MAE: 2.1829, RMSE: 2.7099, R2: -0.0386\n",
      "Epoch 253/1000 - Train Loss: 4.9747, Val Loss: 7.5477, MAE: 2.2075, RMSE: 2.7473, R2: -0.0675\n",
      "Epoch 254/1000 - Train Loss: 4.9614, Val Loss: 7.4229, MAE: 2.1926, RMSE: 2.7245, R2: -0.0498\n",
      "Epoch 255/1000 - Train Loss: 4.8934, Val Loss: 7.4759, MAE: 2.1927, RMSE: 2.7342, R2: -0.0574\n",
      "Epoch 256/1000 - Train Loss: 4.9090, Val Loss: 7.8058, MAE: 2.2380, RMSE: 2.7939, R2: -0.1040\n",
      "Epoch 257/1000 - Train Loss: 4.9313, Val Loss: 7.4275, MAE: 2.2005, RMSE: 2.7253, R2: -0.0505\n",
      "Epoch 258/1000 - Train Loss: 4.8681, Val Loss: 7.4529, MAE: 2.2011, RMSE: 2.7300, R2: -0.0541\n",
      "Epoch 259/1000 - Train Loss: 4.8281, Val Loss: 7.3958, MAE: 2.1847, RMSE: 2.7195, R2: -0.0460\n",
      "Epoch 260/1000 - Train Loss: 4.8074, Val Loss: 7.4353, MAE: 2.1968, RMSE: 2.7268, R2: -0.0516\n",
      "Epoch 261/1000 - Train Loss: 4.8184, Val Loss: 7.4934, MAE: 2.1950, RMSE: 2.7374, R2: -0.0598\n",
      "Epoch 262/1000 - Train Loss: 4.7984, Val Loss: 7.5909, MAE: 2.2165, RMSE: 2.7552, R2: -0.0736\n",
      "Epoch 263/1000 - Train Loss: 4.8004, Val Loss: 7.5918, MAE: 2.2095, RMSE: 2.7553, R2: -0.0737\n",
      "Epoch 264/1000 - Train Loss: 4.7462, Val Loss: 7.3679, MAE: 2.1777, RMSE: 2.7144, R2: -0.0421\n",
      "Epoch 265/1000 - Train Loss: 4.7113, Val Loss: 7.8768, MAE: 2.2415, RMSE: 2.8066, R2: -0.1141\n",
      "Epoch 266/1000 - Train Loss: 4.6853, Val Loss: 7.4900, MAE: 2.1944, RMSE: 2.7368, R2: -0.0593\n",
      "Epoch 267/1000 - Train Loss: 4.7020, Val Loss: 7.4086, MAE: 2.1802, RMSE: 2.7219, R2: -0.0478\n",
      "Epoch 268/1000 - Train Loss: 4.6414, Val Loss: 7.5784, MAE: 2.2168, RMSE: 2.7529, R2: -0.0719\n",
      "Epoch 269/1000 - Train Loss: 4.6537, Val Loss: 7.5942, MAE: 2.2056, RMSE: 2.7558, R2: -0.0741\n",
      "Epoch 270/1000 - Train Loss: 4.6364, Val Loss: 7.5880, MAE: 2.2085, RMSE: 2.7546, R2: -0.0732\n",
      "Epoch 271/1000 - Train Loss: 4.5810, Val Loss: 7.8544, MAE: 2.2351, RMSE: 2.8026, R2: -0.1109\n",
      "Epoch 272/1000 - Train Loss: 4.6471, Val Loss: 7.4174, MAE: 2.1917, RMSE: 2.7235, R2: -0.0491\n",
      "Epoch 273/1000 - Train Loss: 4.5365, Val Loss: 7.9102, MAE: 2.2441, RMSE: 2.8125, R2: -0.1188\n",
      "Epoch 274/1000 - Train Loss: 4.6024, Val Loss: 7.6492, MAE: 2.2118, RMSE: 2.7657, R2: -0.0819\n",
      "Epoch 275/1000 - Train Loss: 4.4822, Val Loss: 7.6445, MAE: 2.2146, RMSE: 2.7649, R2: -0.0812\n",
      "Epoch 276/1000 - Train Loss: 4.5598, Val Loss: 7.6247, MAE: 2.2093, RMSE: 2.7613, R2: -0.0784\n",
      "Epoch 277/1000 - Train Loss: 4.4870, Val Loss: 7.7359, MAE: 2.2163, RMSE: 2.7814, R2: -0.0941\n",
      "Epoch 278/1000 - Train Loss: 4.4039, Val Loss: 7.8183, MAE: 2.2325, RMSE: 2.7961, R2: -0.1058\n",
      "Epoch 279/1000 - Train Loss: 4.5075, Val Loss: 7.8947, MAE: 2.2454, RMSE: 2.8098, R2: -0.1166\n",
      "Epoch 280/1000 - Train Loss: 4.5403, Val Loss: 7.9644, MAE: 2.2528, RMSE: 2.8221, R2: -0.1264\n",
      "Epoch 281/1000 - Train Loss: 4.4499, Val Loss: 7.6522, MAE: 2.2150, RMSE: 2.7663, R2: -0.0823\n",
      "Epoch 282/1000 - Train Loss: 4.4469, Val Loss: 7.7292, MAE: 2.2125, RMSE: 2.7801, R2: -0.0932\n",
      "Epoch 283/1000 - Train Loss: 4.3894, Val Loss: 7.8172, MAE: 2.2284, RMSE: 2.7959, R2: -0.1056\n",
      "Epoch 284/1000 - Train Loss: 4.4148, Val Loss: 7.8497, MAE: 2.2282, RMSE: 2.8017, R2: -0.1102\n",
      "Epoch 285/1000 - Train Loss: 4.3822, Val Loss: 8.0762, MAE: 2.2629, RMSE: 2.8419, R2: -0.1423\n",
      "Epoch 286/1000 - Train Loss: 4.3383, Val Loss: 7.9938, MAE: 2.2529, RMSE: 2.8273, R2: -0.1306\n",
      "Epoch 287/1000 - Train Loss: 4.3513, Val Loss: 7.7875, MAE: 2.2283, RMSE: 2.7906, R2: -0.1014\n",
      "Epoch 288/1000 - Train Loss: 4.3389, Val Loss: 7.7640, MAE: 2.2277, RMSE: 2.7864, R2: -0.0981\n",
      "Epoch 289/1000 - Train Loss: 4.3421, Val Loss: 7.8023, MAE: 2.2260, RMSE: 2.7933, R2: -0.1035\n",
      "Epoch 290/1000 - Train Loss: 4.2857, Val Loss: 7.8549, MAE: 2.2349, RMSE: 2.8027, R2: -0.1110\n",
      "Epoch 291/1000 - Train Loss: 4.3485, Val Loss: 7.8232, MAE: 2.2299, RMSE: 2.7970, R2: -0.1065\n",
      "Epoch 292/1000 - Train Loss: 4.3378, Val Loss: 7.8231, MAE: 2.2241, RMSE: 2.7970, R2: -0.1065\n",
      "Epoch 293/1000 - Train Loss: 4.2974, Val Loss: 7.7908, MAE: 2.2217, RMSE: 2.7912, R2: -0.1019\n",
      "Epoch 294/1000 - Train Loss: 4.3464, Val Loss: 7.7748, MAE: 2.2222, RMSE: 2.7883, R2: -0.0996\n",
      "Epoch 295/1000 - Train Loss: 4.2690, Val Loss: 7.7879, MAE: 2.2231, RMSE: 2.7907, R2: -0.1015\n",
      "Epoch 296/1000 - Train Loss: 4.2972, Val Loss: 7.8756, MAE: 2.2359, RMSE: 2.8064, R2: -0.1139\n",
      "Epoch 297/1000 - Train Loss: 4.3530, Val Loss: 7.8233, MAE: 2.2265, RMSE: 2.7970, R2: -0.1065\n",
      "Epoch 298/1000 - Train Loss: 4.2436, Val Loss: 7.8110, MAE: 2.2273, RMSE: 2.7948, R2: -0.1047\n",
      "Epoch 299/1000 - Train Loss: 4.2395, Val Loss: 7.8519, MAE: 2.2312, RMSE: 2.8021, R2: -0.1105\n",
      "Epoch 300/1000 - Train Loss: 4.2570, Val Loss: 7.8375, MAE: 2.2310, RMSE: 2.7996, R2: -0.1085\n",
      "Epoch 301/1000 - Train Loss: 4.1922, Val Loss: 7.9301, MAE: 2.2387, RMSE: 2.8160, R2: -0.1216\n",
      "Epoch 302/1000 - Train Loss: 4.2338, Val Loss: 7.8800, MAE: 2.2361, RMSE: 2.8071, R2: -0.1145\n",
      "Epoch 303/1000 - Train Loss: 4.2774, Val Loss: 7.8620, MAE: 2.2357, RMSE: 2.8039, R2: -0.1120\n",
      "Epoch 304/1000 - Train Loss: 4.2806, Val Loss: 7.8666, MAE: 2.2353, RMSE: 2.8047, R2: -0.1126\n",
      "Epoch 305/1000 - Train Loss: 4.2042, Val Loss: 7.8809, MAE: 2.2372, RMSE: 2.8073, R2: -0.1146\n",
      "Epoch 306/1000 - Train Loss: 4.2036, Val Loss: 7.8938, MAE: 2.2369, RMSE: 2.8096, R2: -0.1165\n",
      "Epoch 307/1000 - Train Loss: 4.2236, Val Loss: 7.9099, MAE: 2.2404, RMSE: 2.8125, R2: -0.1187\n",
      "Epoch 308/1000 - Train Loss: 4.2615, Val Loss: 7.9090, MAE: 2.2409, RMSE: 2.8123, R2: -0.1186\n",
      "Epoch 309/1000 - Train Loss: 4.2545, Val Loss: 7.9063, MAE: 2.2392, RMSE: 2.8118, R2: -0.1182\n",
      "Epoch 310/1000 - Train Loss: 4.2002, Val Loss: 7.9032, MAE: 2.2390, RMSE: 2.8113, R2: -0.1178\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the pitcher model (example)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Pitcher Model:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m trained_pitcher_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpitcher_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpitcher_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpitcher_loader_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Train the batter model (example)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#print(\"Training Batter Model:\")\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#trained_batter_model = train_model(batter_model, batter_loader, num_epochs=5, lr=5e-3, device='cuda')\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 31\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_dataloader, val_dataloader, num_epochs, lr, device)\u001b[0m\n\u001b[0;32m     28\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 31\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprimary_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopponent_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     33\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\chcro\\Downloads\\mlb_analytics\\src\\model.py:122\u001b[0m, in \u001b[0;36mPitcherTransformerModel.forward\u001b[1;34m(self, pitcher_seq, opp_batter_seq)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pitcher_seq, opp_batter_seq):\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpitcher_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopp_batter_seq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\chcro\\Downloads\\mlb_analytics\\src\\model.py:85\u001b[0m, in \u001b[0;36mCrossAttentionTransformer.forward\u001b[1;34m(self, primary_seq, opponent_seq)\u001b[0m\n\u001b[0;32m     82\u001b[0m opponent_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopponent_embedding(opponent_seq)   \u001b[38;5;66;03m# [batch, seq_len, model_dim]\u001b[39;00m\n\u001b[0;32m     84\u001b[0m primary_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprimary_encoder(primary_emb)    \u001b[38;5;66;03m# [batch, seq_len, model_dim]\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m opponent_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopponent_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopponent_emb\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# [batch, seq_len, model_dim]\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Apply additional cross attention blocks\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_blocks:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:511\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    508\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 511\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    519\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.0\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:904\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    900\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    902\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[0;32m    903\u001b[0m         x\n\u001b[1;32m--> 904\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    905\u001b[0m     )\n\u001b[0;32m    906\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:918\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\n\u001b[0;32m    912\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    913\u001b[0m     x: Tensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    916\u001b[0m     is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    917\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 918\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\activation.py:1368\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1343\u001b[0m         query,\n\u001b[0;32m   1344\u001b[0m         key,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m   1366\u001b[0m     )\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1368\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1388\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\functional.py:6163\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   6158\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m bias_v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   6160\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   6161\u001b[0m \u001b[38;5;66;03m# reshape q, k, v for multihead attention and make them batch first\u001b[39;00m\n\u001b[0;32m   6162\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m-> 6163\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   6164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m static_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   6165\u001b[0m     k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mview(k\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], bsz \u001b[38;5;241m*\u001b[39m num_heads, head_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the pitcher model (example)\n",
    "print(\"Training Pitcher Model:\")\n",
    "trained_pitcher_model = train_model(\n",
    "    pitcher_model, \n",
    "    pitcher_loader, \n",
    "    pitcher_loader_val, \n",
    "    num_epochs=1000, \n",
    "    lr=5e-4, \n",
    "    device='cuda')\n",
    "\n",
    "# Train the batter model (example)\n",
    "#print(\"Training Batter Model:\")\n",
    "#trained_batter_model = train_model(batter_model, batter_loader, num_epochs=5, lr=5e-3, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first few samples from the batter dataset\n",
    "for i in range(3):\n",
    "    batter_seq, opp_pitcher_seq, target = pitcher_dataset[i]\n",
    "    print(f\"Sample {i+1}\")\n",
    "    print(\"pitcher Sequence:\")\n",
    "    print(batter_seq)\n",
    "    print(\"Opposing batter Sequence:\")\n",
    "    print(opp_pitcher_seq)\n",
    "    print(\"Target:\")\n",
    "    print(target)\n",
    "    print(\"===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
